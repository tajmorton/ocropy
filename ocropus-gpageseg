#!/usr/bin/env python

# TODO:
# ! add option for padding
# - fix occasionally missing page numbers
# - treat large h-whitespace as separator
# - handle overlapping candidates
# - use cc distance statistics instead of character scale
# - page frame detection
# - read and use text image segmentation mask
# - pick up stragglers
# ? laplacian as well

import logging

from pylab import *
import argparse,glob,os,os.path
import traceback
from scipy.ndimage import measurements
from scipy.misc import imsave
from scipy.ndimage.filters import gaussian_filter,uniform_filter,maximum_filter
from multiprocessing import Pool
import ocrolib
from ocrolib import psegutils,morph,sl
from ocrolib.toplevel import *

from ocrolib.common import raise_for_image_errors
from ocrolib.common import raise_for_image_errors

logger = logging.getLogger('ocropus.gpagseg')

def norm_max(v):
    return v/amax(v)

def B(a):
    if a.dtype == dtype('B'):
        return a

    return array(a, 'B')

def DSAVE(title, image, output_dir=None):
    if not output_dir:
        return

    if type(image) == list:
        assert len(image)==3
        image = transpose(array(image), [1,2,0])

    fname = os.path.join(output_dir, "_"+title+".png")
    logger.debug("%s -> %s" % (title, fname))
    imsave(fname, image)


################################################################
### Column finding.
###
### This attempts to find column separators, either as extended
### vertical black lines or extended vertical whitespace.
### It will work fairly well in simple cases, but for unusual
### documents, you need to tune the parameters.
################################################################

def compute_separators_morph(binary, scale, sep_width_increase, max_seps):
    """Finds vertical black lines corresponding to column separators."""
    d0 = int(max(5,scale/4))
    d1 = int(max(5,scale))+sep_width_increase
    thick = morph.r_dilation(binary,(d0,d1))
    vert = morph.rb_opening(thick,(10*scale,1))
    vert = morph.r_erosion(vert,(d0//2,sep_width_increase))
    vert = morph.select_regions(vert,sl.dim1,min=3,nbest=2*maxseps)
    vert = morph.select_regions(vert,sl.dim0,min=20*scale,nbest=maxseps)
    return vert

def compute_colseps_morph(binary, scale, min_aspect, min_height, max_seps):
    """Finds extended vertical whitespace corresponding to column separators
    using morphological operations."""
    boxmap = psegutils.compute_boxmap(binary,scale,dtype='B')
    bounds = morph.rb_closing(B(boxmap),(int(5*scale),int(5*scale)))
    bounds = maximum(B(1-bounds),B(boxmap))
    cols = 1-morph.rb_closing(boxmap,(int(20*scale),int(scale)))
    cols = morph.select_regions(cols,sl.aspect,min=min_aspect)
    cols = morph.select_regions(cols,sl.dim0,min=min_height*scale,nbest=max_seps)
    cols = morph.r_erosion(cols,(int(0.5+scale),0))
    cols = morph.r_dilation(cols,(int(0.5+scale),0),origin=(int(scale/2)-1,0))
    return cols

def compute_colseps_mconv(binary, min_height, max_seps, scale=1.0, progress_dir=None):
    """Find column separators using a combination of morphological
    operations and convolution."""
    h, w = binary.shape
    smoothed = gaussian_filter(1.0*binary,(scale,scale*0.5))
    smoothed = uniform_filter(smoothed,(5.0*scale,1))
    thresh = (smoothed<amax(smoothed)*0.1)
    DSAVE("1thresh",thresh, progress_dir)
    blocks = morph.rb_closing(binary,(int(4*scale),int(4*scale)))
    DSAVE("2blocks",blocks, progress_dir)
    seps = minimum(blocks,thresh)
    seps = morph.select_regions(seps,sl.dim0,min=min_height,nbest=max_seps)
    DSAVE("3seps",seps, progress_dir)
    blocks = morph.r_dilation(blocks,(5,5))
    DSAVE("4blocks",blocks, progress_dir)
    seps = maximum(seps,1-blocks)
    DSAVE("5combo",seps, progress_dir)
    return seps

def compute_colseps_conv(binary, min_height, max_seps, scale=1.0, progress_dir=None):
    """Find column separators by convoluation and
    thresholding."""
    h, w = binary.shape
    # find vertical whitespace by thresholding
    smoothed = gaussian_filter(1.0*binary,(scale,scale*0.5))
    smoothed = uniform_filter(smoothed,(5.0*scale,1))
    thresh = (smoothed<amax(smoothed)*0.1)
    DSAVE("1thresh",thresh, progress_dir)
    # find column edges by filtering
    grad = gaussian_filter(1.0*binary,(scale,scale*0.5),order=(0,1))
    grad = uniform_filter(grad,(10.0*scale,1))
    # grad = abs(grad) # use this for finding both edges
    grad = (grad>0.5*amax(grad))
    DSAVE("2grad",grad, progress_dir)
    # combine edges and whitespace
    seps = minimum(thresh,maximum_filter(grad,(int(scale),int(5*scale))))
    seps = maximum_filter(seps,(int(2*scale),1))
    DSAVE("3seps",seps, progress_dir)
    # select only the biggest column separators
    seps = morph.select_regions(seps,sl.dim0,min=min_height*scale,nbest=max_seps+1)
    DSAVE("4seps",seps, progress_dir)
    return seps

def compute_colseps(binary, scale, black_seps_width_increase=None, progress_dir=None):
    """Computes column separators either from vertical black lines or whitespace."""
    colseps = compute_colseps_conv(binary, scale, progress_dir)
    DSAVE("colwsseps", 0.7*colseps+0.3*binary, progress_dir)

    if black_seps_width_increase:
        seps = compute_separators_morph(binary, scale, black_sep_width_increase)
        DSAVE("colseps", 0.7*seps+0.3*binary, progress_dir)

        #colseps = compute_colseps_morph(binary,scale)
        colseps = maximum(colseps, seps)
        binary = minimum(binary, 1 - seps)

    return colseps, binary


################################################################
### Text Line Finding.
###
### This identifies the tops and bottoms of text lines by
### computing gradients and performing some adaptive thresholding.
### Those components are then used as seeds for the text lines.
################################################################

def compute_gradmaps(binary, scale, hscale, vscale, use_gauss, progress_dir=None):
    # use gradient filtering to find baselines
    boxmap = psegutils.compute_boxmap(binary, scale)
    cleaned = boxmap*binary
    DSAVE("cleaned", cleaned, progress_dir)

    if usegauss:
        # this uses Gaussians
        grad = gaussian_filter(1.0*cleaned,(vscale*0.3*scale,
                                            hscale*6*scale),order=(1,0))
    else:
        # this uses non-Gaussian oriented filters
        grad = gaussian_filter(1.0*cleaned,(max(4,vscale*0.3*scale),
                                            hscale*scale),order=(1,0))
        grad = uniform_filter(grad,(vscale,hscale*6*scale))
    bottom = ocrolib.norm_max((grad<0)*(-grad))
    top = ocrolib.norm_max((grad>0)*grad)
    return bottom, top, boxmap

def compute_line_seeds(binary,bottom,top,colseps,scale, threshold, vscale, progress_dir=None):
    """Base on gradient maps, computes candidates for baselines
    and xheights.  Then, it marks the regions between the two
    as a line seed."""
    t = threshold
    vrange = int(vscale*scale)
    bmarked = maximum_filter(bottom==maximum_filter(bottom,(vrange,0)),(2,2))
    bmarked *= (bottom>t*amax(bottom)*t)*(1-colseps)
    tmarked = maximum_filter(top==maximum_filter(top,(vrange,0)),(2,2))
    tmarked *= (top>t*amax(top)*t/2)*(1-colseps)
    tmarked = maximum_filter(tmarked,(1,20))
    seeds = zeros(binary.shape,'i')
    delta = max(3,int(scale/2))
    for x in range(bmarked.shape[1]):
        transitions = sorted([(y,1) for y in find(bmarked[:,x])]+[(y,0) for y in find(tmarked[:,x])])[::-1]
        transitions += [(0,0)]
        for l in range(len(transitions)-1):
            y0, s0 = transitions[l]
            if s0 == 0:
                continue
            seeds[y0-delta:y0,x] = 1

            y1, s1 = transitions[l+1]
            if s1 == 0 and (y0-y1) < 5*scale:
                seeds[y1:y0,x] = 1
    seeds = maximum_filter(seeds,(1,int(1+scale)))
    seeds *= (1-colseps)
    DSAVE("lineseeds",[seeds,0.3*tmarked+0.7*bmarked,binary], progress_dir)
    seeds,_ = morph.label(seeds)
    return seeds


################################################################
### The complete line segmentation process.
################################################################

def remove_hlines(binary, scale, maxsize=10):
    labels, _ = morph.label(binary)
    objects = morph.find_objects(labels)
    for i, b in enumerate(objects):
        if sl.width(b) > maxsize*scale:
            labels[b][labels[b]==i+1] = 0
    return array(labels != 0, 'B')

def compute_segmentation(binary,
                         scale,
                         use_gauss,
                         black_seps_width_increase,
                         colseps_max,
                         baseline_threshold,
                         progress_dir=None):
    """Given a binary image, compute a complete segmentation into
    lines, computing both columns and text lines."""
    binary = array(binary, 'B')

    # start by removing horizontal black lines, which only
    # interfere with the rest of the page segmentation
    binary = remove_hlines(binary, scale)

    # do the column finding
    logger.info("Computing column separators")
    colseps, binary = compute_colseps(binary, scale,
                                      black_seps_width_increase, progress_dir)

    # now compute the text line seeds
    logger.info("Computing lines")
    bottom, top, boxmap = compute_gradmaps(binary, scale, use_gauss, progress_dir)
    seeds = compute_line_seeds(binary, bottom, top, colseps_max,
                               baseline_threshold, scale, progress_dir)
    DSAVE("seeds", [bottom,top,boxmap], progress_dir)

    # spread the text line seeds to all the remaining
    # components
    logger.info("Propagating labels")
    llabels = morph.propagate_labels(boxmap, seeds, conflict=0)

    logger.info("Spreading labels")
    spread = morph.spread_labels(seeds, maxdist=scale)
    llabels = where(llabels > 0, llabels, spread*binary)
    segmentation = llabels * binary
    return segmentation


def segment_image(binary_image,
                  gaussian_baselines=False,
                  black_seps_width_increase=10,
                  black_seps_max=2,
                  colseps_max=2,
                  colsep_min_aspect=1.1,
                  colsep_min_height=10,
                  baseline_threshold=0.2,
                  progress_dir=None,
                  validate_image=False,
                  document_scale=0,
                  document_hscale=1.0,
                  document_vscale=1.0,
                  document_min_scale=12.0,
                  document_max_lines=300,
                  component_noise_threshold=8,
                  line_padding=3,
                  line_mask_expand=3):
    checktype(binary_image, ABINARY2)
 
    if validate_image: 
        raise_for_image_errors(binary_image, connected_components=True)

    binary_image = 1 - binary_image  # invert

    if document_scale == 0:
        scale = psegutils.estimate_scale(binary_image)
    else:
        scale = document_scale 

    logger.info("Document scale {scale}".format(scale=scale))
    if isnan(scale) or scale > 1000.0:
        raise DocumentScaleException("Bad scale ({})".format(
            scale))

    if scale < document_min_scale:
        raise DocumentScaleError("Scale ({}) less than document_min_scale "
                                 "({}).".format(scale, min_scale))

    # find columns and text lines
    logger.info("Computing segmentation")
    segmentation = compute_segmentation(
        binary_image,
        scale=scale,
        use_gauss=gaussian_baselines, 
        black_seps_width_increase=black_seps_width_increase,
        colseps_max=colseps_max,
        baseline_threshold=baseline_threshold,
        progress_dir=progress_dir)

    if amax(segmentation) > document_max_lines: 
        raise DocumentSegmentationException(
            "Number of lines exceeded ({}) "
            "document_max_lines ({})".format(
                amax(segmentation),
                document_max_lines))

    logger.info("Number of lines: %d" % amax(segmentation))

    # compute the reading order
    logger.info("Finding reading order")
    lines = psegutils.compute_lines(segmentation, scale)
    order = psegutils.reading_order([l.bounds for l in lines])
    lsort = psegutils.topsort(order)

    # renumber the labels so that they conform to the specs
    nlabels = amax(segmentation) + 1
    renumber = zeros(nlabels, 'i')
    for i, v in enumerate(lsort):
        renumber[lines[v].label] = 0x010000 + (i+1)
    segmentation = renumber[segmentation]

    lines = [lines[i] for i in lsort]
    cleaned = ocrolib.remove_noise(binary, component_noise_threshold)
    segmented_images=[]
    for i, l in enumerate(lines):
        segmented_images.append(
            psegutils.extract_masked(1-cleaned,l,pad=line_padding,expand=line_mask_expand))

    return lines, segmented_images, segmentation

################################################################
### Processing each file.
################################################################

def process1(job):
    fname, i, gray, config = job
    global base
    base, _ = ocrolib.allsplitext(fname)
    outputdir = base

    try:
        binary = ocrolib.read_image_binary(base+".bin.png")
    except IOError:
        try:
            binary = ocrolib.read_image_binary(fname)
        except IOError as err:
            raise err

    if gray:
        if os.path.exists(base+".nrm.png"):
            gray = ocrolib.read_image_gray(base+".nrm.png")
        checktype(gray,GRAYSCALE)


    # finally, output everything
    lines, segmented_images, segmentation = segment_image(binary, **config)

    logger.info("Writing lines")
    if not os.path.exists(outputdir):
        os.mkdir(outputdir)
    ocrolib.write_page_segmentation("%s.pseg.png" % outputdir, segmentation)

    for i, line_image in enumerate(line_images):
        ocrolib.write_image_binary("%s/01%04x.bin.png" % (outputdir, i+1), line_image)

    if gray:
        grayline = psegutils.extract_masked(gray, l,
                                            pad=config['line_padding'],
                                            expand=config['line_mask_expand'])
        ocrolib.write_image_gray("%s/01%04x.nrm.png"%(outputdir,i+1),grayline)

    logger.info("{fname} - Found {lines} lines (scale {scale})",format(
        fname=fname, lines=lines, scale=scale))

def safe_process1(job):
    fname,i = job
    try:
        process1(job)
    except ocrolib.OcropusException as e:
        if e.trace:
            traceback.print_exc()
        else:
            print fname,":",e
    except Exception as e:
        traceback.print_exc()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # error checking
    parser.add_argument('-n','--nocheck',action="store_true",
                        help="disable error checking on inputs")

    parser.add_argument('-z','--zoom',type=float,default=0.5,
                        help='zoom for page background estimation, smaller=faster')

    parser.add_argument('--gray',action='store_true',
                        help='output grayscale lines as well (%(default)s)')
    parser.add_argument('-q','--quiet',action='store_true',
                        help='be less verbose (%(default)s)')

    # limits
    parser.add_argument('--minscale',type=float,default=12.0,
                        help='minimum scale permitted (%(default)s)')
    parser.add_argument('--maxlines',type=float,default=300,
                        help='maximum # lines permitted (%(default)s)')

    # scale parameters
    parser.add_argument('--scale',type=float,default=0.0,
                        help='the basic scale of the document (roughly, xheight) 0=automatic (%(default)s)')
    parser.add_argument('--hscale',type=float,default=1.0,
                        help='non-standard scaling of horizontal parameters (%(default)s)')
    parser.add_argument('--vscale',type=float,default=1.0,
                        help='non-standard scaling of vertical parameters (%(default)s)')

    # line parameters
    parser.add_argument('--threshold',type=float,default=0.2,
                        help='baseline threshold (%(default)s)')
    parser.add_argument('--noise',type=int,default=8,
                        help="noise threshold for removing small components from lines (%(default)s)")
    parser.add_argument('--usegauss',action='store_true',
                        help='use gaussian instead of uniform (%(default)s)')

    # column parameters
    parser.add_argument('--maxseps',type=int,default=2,
                        help='maximum black column separators (%(default)s)')
    parser.add_argument('--sepwiden',type=int,default=10,
                        help='widen black separators (to account for warping) (%(default)s)')
    parser.add_argument('-b','--blackseps',action="store_true",
                        help="also check for black column separators")

    # whitespace column separators
    parser.add_argument('--maxcolseps',type=int,default=2,
                        help='maximum # whitespace column separators (%(default)s)')
    parser.add_argument('--csminaspect',type=float,default=1.1,
                        help='minimum aspect ratio for column separators')
    parser.add_argument('--csminheight',type=float,default=10,
                        help='minimum column height (units=scale) (%(default)s)')

    parser.add_argument('-p','--pad',type=int,default=3,
                        help='padding for extracted lines (%(default)s)')
    parser.add_argument('-e','--expand',type=int,default=3,
                        help='expand mask for grayscale extraction (%(default)s)')
    parser.add_argument('-Q','--parallel',type=int,default=0,
                        help="number of CPUs to use")
    parser.add_argument('-d','--debug', type=str, default=None, metavar='DEBUG_DIR',
                        help="Output intermediate images")
    parser.add_argument('files',nargs='+')
    args = parser.parse_args()
    args.files = ocrolib.glob_all(args.files)

    if len(args.files) < 1:
        parser.print_help()
        sys.exit(0)

    print 
    print "#"*10,(" ".join(sys.argv))[:60]
    print 

    if args.parallel > 1 and args.debug:
        logger.warning("Cannot output intermediate images with "
                       "multiple threads.")
        args.debug = None

    if len(args.files) == 1 and os.path.isdir(args.files[0]):
        files = glob.glob(args.files[0]+"/????.png")
    else:
        files = args.files

    if args.debug and not os.path.exists(args.debug):
        os.mkdir(args.debug)

    segmentation_config = {
        "progress_dir": args.debug,
        "validate_image": not args.nocheck,
        "document_scale": args.scale,
        "document_hscale": args.hscale,
        "document_vscale": args.vscale,
        "document_min_scale": args.minscale,
        "document_max_lines": args.maxlines,
        "component_noise_threshold": args.noise,
        "line_padding": args.pad,
        "line_mask_expand": args.expand,
        "max_seps": args.maxcolseps,
        "black_seps_width_increase": None if not args.blackseps else args.sepwiden, 
        "colsep_min_aspect": args.csminaspect,
        "colsep_min_height": args.csminheight,
        "baseline_threshold": args.threshold,
        "gaussian_baselines": args.use_gauss,
    }

    if args.parallel < 2:
        count = 0
        for i, f in enumerate(files):
            if args.parallel == 0:
                print f
            count += 1
            safe_process1((f,i+1, segmentation_config))
    else:
        pool = Pool(processes=args.parallel)
        jobs = []
        for i, f in enumerate(files):
            jobs += [(f,i+1, segmentation_config)]
        result = pool.map(process1,jobs)
